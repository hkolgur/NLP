{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Test word 2 vec corpus construciton before we apply to all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>VY_tvNUCCXGXQeSvJl757Q</td>\n",
       "      <td>2012-07-28</td>\n",
       "      <td>Ubyfp2RSDYW0g7Mbr8N3iA</td>\n",
       "      <td>3</td>\n",
       "      <td>First visit...Had lunch here today - used my G...</td>\n",
       "      <td>review</td>\n",
       "      <td>_eqQoPtQ3e3UxLE4faT6ow</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>EKzMHI1tip8rC1-ZAy64yg</td>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>2XyIOQKbVFb6uXQdJ0RzlQ</td>\n",
       "      <td>4</td>\n",
       "      <td>Should be called house of deliciousness!\\n\\nI ...</td>\n",
       "      <td>review</td>\n",
       "      <td>ROru4uk5SaYc3rg8IU7SQw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>53YGfwmbW73JhFiemNeyzQ</td>\n",
       "      <td>2010-11-16</td>\n",
       "      <td>jyznYkIbpqVmlsZxSDSypA</td>\n",
       "      <td>4</td>\n",
       "      <td>I recently visited Olive and Ivy for business ...</td>\n",
       "      <td>review</td>\n",
       "      <td>gGbN1aKQHMgfQZkqlsuwzg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9SKdOoDHcFoxK5ZtsgHJoA</td>\n",
       "      <td>2012-12-02</td>\n",
       "      <td>5UKq9WQE1qQbJ0DJbc-B6Q</td>\n",
       "      <td>2</td>\n",
       "      <td>My nephew just moved to Scottsdale recently so...</td>\n",
       "      <td>review</td>\n",
       "      <td>0lyVoNazXa20WzUyZPLaQQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>pF7uRzygyZsltbmVpjIyvw</td>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>vWSmOhg2ID1MNZHaWapGbA</td>\n",
       "      <td>5</td>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>review</td>\n",
       "      <td>KSBFytcdjPKZgXKQnYQdkA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "9995  VY_tvNUCCXGXQeSvJl757Q  2012-07-28  Ubyfp2RSDYW0g7Mbr8N3iA      3   \n",
       "9996  EKzMHI1tip8rC1-ZAy64yg  2012-01-18  2XyIOQKbVFb6uXQdJ0RzlQ      4   \n",
       "9997  53YGfwmbW73JhFiemNeyzQ  2010-11-16  jyznYkIbpqVmlsZxSDSypA      4   \n",
       "9998  9SKdOoDHcFoxK5ZtsgHJoA  2012-12-02  5UKq9WQE1qQbJ0DJbc-B6Q      2   \n",
       "9999  pF7uRzygyZsltbmVpjIyvw  2010-10-16  vWSmOhg2ID1MNZHaWapGbA      5   \n",
       "\n",
       "                                                   text    type  \\\n",
       "9995  First visit...Had lunch here today - used my G...  review   \n",
       "9996  Should be called house of deliciousness!\\n\\nI ...  review   \n",
       "9997  I recently visited Olive and Ivy for business ...  review   \n",
       "9998  My nephew just moved to Scottsdale recently so...  review   \n",
       "9999  4-5 locations.. all 4.5 star average.. I think...  review   \n",
       "\n",
       "                     user_id  cool  useful  funny  \n",
       "9995  _eqQoPtQ3e3UxLE4faT6ow     1       2      0  \n",
       "9996  ROru4uk5SaYc3rg8IU7SQw     0       0      0  \n",
       "9997  gGbN1aKQHMgfQZkqlsuwzg     0       0      0  \n",
       "9998  0lyVoNazXa20WzUyZPLaQQ     0       0      0  \n",
       "9999  KSBFytcdjPKZgXKQnYQdkA     0       0      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('yelp.csv')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a sample list to unit test the preprocessed_reviews code below\n",
    "sample_10=BOW.head(2)\n",
    "sample_10['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm is for printing the status bar\n",
    "from tqdm import tqdm\n",
    "preprocessed_reviews = []\n",
    "for sentence in tqdm(sample_10['text'].values):\n",
    "    sentence=clean_review_text(sentence)\n",
    "   # sentence=' '.join(sentence)\n",
    "   # preprocessed_reviews.append(sentence.strip())\n",
    "    preprocessed_reviews.append(sentence)\n",
    "print(preprocessed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "def clean_review_text(review):\n",
    "    \"\"\"\n",
    "    1. Remove Punctuation\n",
    "    2. Remove Stop Words\n",
    "    3. Apply SnowBall Stemmer to remove morphological affixes from words, leaving only the word stem.\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    no_punc=[c for c in review if c not in string.punctuation]\n",
    "    no_punc=''.join(no_punc)\n",
    "    return [stemmer.stem(word) for word in no_punc.split() if word.lower() not in stopwords.words('english')]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Word2Vec on whole data and build your own corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own Word2Vec model using your own text corpus for whole data\n",
    "# tqdm is for printing the status bar\n",
    "from tqdm import tqdm\n",
    "preprocessed_reviews = []\n",
    "for sentence in tqdm(BOW['text'].values):\n",
    "    sentence=clean_review_text(sentence)\n",
    "    preprocessed_reviews.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Precict the simillarity of wrods with the model we have built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "# min_count = 2 considers only words that occured atleast 2 times\n",
    "w2v_model=Word2Vec(preprocessed_reviews,min_count=2,size=50)\n",
    "print(w2v_model.wv.most_similar('great'))\n",
    "print('='*50)\n",
    "print(w2v_model.wv.most_similar('worst'))\n",
    "print(w2v_model.wv['worst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create list of all the words/Corpus form the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "print(\"number of words that occured minimum 2 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert text into Vector using Average W2Vec  for whole sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each rating.\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in tqdm(preprocessed_reviews): # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/ \n",
    "from tqdm import tqdm\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec[0])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "        for sent in tqdm(X): # for each review/sentence\n",
    "            sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "            cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "            for word in sent: # for each word in a review/sentence\n",
    "                if word in w2v_words:\n",
    "                    vec = w2v_model.wv[word]\n",
    "                    sent_vec += vec\n",
    "                    cnt_words += 1\n",
    "            if cnt_words != 0:\n",
    "                sent_vec /= cnt_words\n",
    "            sent_vectors.append(sent_vec)\n",
    "        return sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_W2V=Pipeline([('MeanTransformer',MeanEmbeddingVectorizer(w2v_model)),\n",
    "#                       ('tfidf',TfidfTransformer()),\n",
    "                       ('algorithm',RandomForestClassifier())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data[(data['stars']==5 )| (data['stars']==1)]\n",
    "X_W2V= df['text']\n",
    "y_W2V=df['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take input text and clean punctuations , stop words etc with custom analyzer\n",
    "cvo=CountVectorizer(analyzer=clean_review_text)\n",
    "#Replace X with the cleaned text\n",
    "X_W2V=cvo.fit_transform(X_W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w2v_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d04a3ab92960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train_W2V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_W2V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_W2V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_W2V\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_W2V\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_W2V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMeanEmbeddingVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_W2V\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msent_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_W2V\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-15cd5d7b109b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mcnt_words\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m# num of words with a valid vector in the sentence/review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for each word in a review/sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0msent_vec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v_words' is not defined"
     ]
    }
   ],
   "source": [
    "sent_vectors=[[]]\n",
    "X_train_W2V, X_test_W2V, y_train_W2V, y_test_W2V = train_test_split(X_W2V,y_W2V, test_size=0.3, random_state=101)\n",
    "model=MeanEmbeddingVectorizer(X_train_W2V)\n",
    "sent_vectors=model.transform(X_train_W2V)\n",
    "print(sent_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_W2V, X_test_W2V, y_train_W2V, y_test_W2V = train_test_split(X_W2V,y_W2V, test_size=0.3, random_state=101)\n",
    "#scores.append(accuracy_score(model.fit(X_train, y_train).predict(X_test), y_test))\n",
    "pipeline_W2V.fit(X_W2V, y_W2V)\n",
    "pipeline_W2V.transform(X_W2V, y_W2V)\n",
    "predictions_W2V=pipeline_W2V.predict(\"BOW[0]\")\n",
    "print(predictions_W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_W2V=pipeline_W2V.predict(\"U can go there n check the car out. If u wanna buy 1 there? That's wrong move! If u even want a car service from there? U made a biggest mistake of ur life!! I had 1 time asked my girlfriend to take my car there for an oil service, guess what? They ripped my girlfriend off by lying how bad my car is now. If without fixing the problem. Might bring some serious accident. Then she did what they said. 4 brand new tires, timing belt, 4 new brake pads. U know why's the worst? All of those above I had just changed 2 months before!!! What a trashy dealer is that? People, better off go somewhere!\")\n",
    "print(predictions_W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_W2V[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "# first distance should be much smaller than second\n",
    "euclidean(w2v_model['wife'], w2v_model['girlfriend']), euclidean(w2v_model['wife'], w2v_model['problem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean(w2v_model['problem'], w2v_model['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp=MeanEmbeddingVectorizer(BOW[0])\n",
    "mp.transform()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec_tr = MeanEmbeddingVectorizer(w2v_model)\n",
    "doc_vec = mean_vec_tr.transform(w2v_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/ \n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "# this is a very toy example, do not try this at home unless you want to understand the usage differences\n",
    "docs=[\"the house had a tiny little mouse\",\n",
    "      \"the cat saw the mouse\",\n",
    "      \"the mouse ran away from the house\",\n",
    "      \"the cat finally ate the mouse\",\n",
    "      \"the end of the mouse story\"\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ate',\n",
       " 'away',\n",
       " 'cat',\n",
       " 'end',\n",
       " 'finally',\n",
       " 'from',\n",
       " 'had',\n",
       " 'house',\n",
       " 'little',\n",
       " 'mouse',\n",
       " 'of',\n",
       " 'ran',\n",
       " 'saw',\n",
       " 'story',\n",
       " 'the',\n",
       " 'tiny']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 2, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector[:].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.fit(word_count_vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 2.09861229, 1.69314718, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229, 1.69314718, 2.09861229, 1.        ,\n",
       "       2.09861229, 2.09861229, 2.09861229, 2.09861229, 1.        ,\n",
       "       2.09861229])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector=cv.transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 2, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector[:].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.49356209, 0.39820278, 0.49356209, 0.23518498,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.23518498,\n",
       "        0.49356209],\n",
       "       [0.        , 0.        , 0.48334378, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.28547062,\n",
       "        0.        , 0.        , 0.59909216, 0.        , 0.57094124,\n",
       "        0.        ],\n",
       "       [0.        , 0.45709287, 0.        , 0.        , 0.        ,\n",
       "        0.45709287, 0.        , 0.36877965, 0.        , 0.2178072 ,\n",
       "        0.        , 0.45709287, 0.        , 0.        , 0.43561441,\n",
       "        0.        ],\n",
       "       [0.51392301, 0.        , 0.41462985, 0.        , 0.51392301,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.24488707,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.48977413,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.49175319, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.23432303,\n",
       "        0.49175319, 0.        , 0.        , 0.49175319, 0.46864606,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector[:].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "S = ['wife',\n",
    " 'took',\n",
    " 'birthday',\n",
    " 'breakfast',\n",
    " 'excel',\n",
    " 'weather',\n",
    " 'perfect',\n",
    " 'made',\n",
    " 'sit',\n",
    " 'outsid',\n",
    " 'overlook']\n",
    "model = TfidfVectorizer()\n",
    "model.fit(S)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'birthday': 2.791759469228055,\n",
       " 'breakfast': 2.791759469228055,\n",
       " 'excel': 2.791759469228055,\n",
       " 'made': 2.791759469228055,\n",
       " 'outsid': 2.791759469228055,\n",
       " 'overlook': 2.791759469228055,\n",
       " 'perfect': 2.791759469228055,\n",
       " 'sit': 2.791759469228055,\n",
       " 'took': 2.791759469228055,\n",
       " 'weather': 2.791759469228055,\n",
       " 'wife': 2.791759469228055}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=[[\"ant\",\"bed\"],[\"cat\",\"den\"]]\n",
    "' '.join(s1[ele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_str="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ant bed', 'cat den']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(' '.join,s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-f75276b09852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-f75276b09852>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "for ele in s1  lambda el: \n",
    "    new=[' '.join(s1[ele])\n",
    "    \n",
    "print(new)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
