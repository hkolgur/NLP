{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Yelp reviews data set and check the contents of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>VY_tvNUCCXGXQeSvJl757Q</td>\n",
       "      <td>2012-07-28</td>\n",
       "      <td>Ubyfp2RSDYW0g7Mbr8N3iA</td>\n",
       "      <td>3</td>\n",
       "      <td>First visit...Had lunch here today - used my G...</td>\n",
       "      <td>review</td>\n",
       "      <td>_eqQoPtQ3e3UxLE4faT6ow</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>EKzMHI1tip8rC1-ZAy64yg</td>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>2XyIOQKbVFb6uXQdJ0RzlQ</td>\n",
       "      <td>4</td>\n",
       "      <td>Should be called house of deliciousness!\\n\\nI ...</td>\n",
       "      <td>review</td>\n",
       "      <td>ROru4uk5SaYc3rg8IU7SQw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>53YGfwmbW73JhFiemNeyzQ</td>\n",
       "      <td>2010-11-16</td>\n",
       "      <td>jyznYkIbpqVmlsZxSDSypA</td>\n",
       "      <td>4</td>\n",
       "      <td>I recently visited Olive and Ivy for business ...</td>\n",
       "      <td>review</td>\n",
       "      <td>gGbN1aKQHMgfQZkqlsuwzg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9SKdOoDHcFoxK5ZtsgHJoA</td>\n",
       "      <td>2012-12-02</td>\n",
       "      <td>5UKq9WQE1qQbJ0DJbc-B6Q</td>\n",
       "      <td>2</td>\n",
       "      <td>My nephew just moved to Scottsdale recently so...</td>\n",
       "      <td>review</td>\n",
       "      <td>0lyVoNazXa20WzUyZPLaQQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>pF7uRzygyZsltbmVpjIyvw</td>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>vWSmOhg2ID1MNZHaWapGbA</td>\n",
       "      <td>5</td>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>review</td>\n",
       "      <td>KSBFytcdjPKZgXKQnYQdkA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "9995  VY_tvNUCCXGXQeSvJl757Q  2012-07-28  Ubyfp2RSDYW0g7Mbr8N3iA      3   \n",
       "9996  EKzMHI1tip8rC1-ZAy64yg  2012-01-18  2XyIOQKbVFb6uXQdJ0RzlQ      4   \n",
       "9997  53YGfwmbW73JhFiemNeyzQ  2010-11-16  jyznYkIbpqVmlsZxSDSypA      4   \n",
       "9998  9SKdOoDHcFoxK5ZtsgHJoA  2012-12-02  5UKq9WQE1qQbJ0DJbc-B6Q      2   \n",
       "9999  pF7uRzygyZsltbmVpjIyvw  2010-10-16  vWSmOhg2ID1MNZHaWapGbA      5   \n",
       "\n",
       "                                                   text    type  \\\n",
       "9995  First visit...Had lunch here today - used my G...  review   \n",
       "9996  Should be called house of deliciousness!\\n\\nI ...  review   \n",
       "9997  I recently visited Olive and Ivy for business ...  review   \n",
       "9998  My nephew just moved to Scottsdale recently so...  review   \n",
       "9999  4-5 locations.. all 4.5 star average.. I think...  review   \n",
       "\n",
       "                     user_id  cool  useful  funny  \n",
       "9995  _eqQoPtQ3e3UxLE4faT6ow     1       2      0  \n",
       "9996  ROru4uk5SaYc3rg8IU7SQw     0       0      0  \n",
       "9997  gGbN1aKQHMgfQZkqlsuwzg     0       0      0  \n",
       "9998  0lyVoNazXa20WzUyZPLaQQ     0       0      0  \n",
       "9999  KSBFytcdjPKZgXKQnYQdkA     0       0      0  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('yelp.csv')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom analyzer to clean punctuation, stopwords, numerics, stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "def clean_review_text(review):\n",
    "    \"\"\"\n",
    "    1. Remove Punctuation\n",
    "    2. Remove Stop Words\n",
    "    3. Apply SnowBall Stemmer to remove morphological affixes from words, leaving only the word stem.\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    no_punc=[c for c in review if c not in string.punctuation]\n",
    "    no_punc=''.join(no_punc)\n",
    "    return [stemmer.stem(word) for word in no_punc.split() if word.lower() not in stopwords.words('english') and word.isalpha()]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean each review using custom analyzer. This cleaned text will be list of lists for all the reviews , which can  later be used to create our own corpus for W2V Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:03<00:00, 32.98it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tqdm is for printing the status bar\n",
    "from tqdm import tqdm\n",
    "preprocessed_reviews = []\n",
    "for sentence in tqdm(data['text'].values):\n",
    "    sentence=clean_review_text(sentence)\n",
    "    preprocessed_reviews.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wife',\n",
       "  'took',\n",
       "  'birthday',\n",
       "  'breakfast',\n",
       "  'excel',\n",
       "  'weather',\n",
       "  'perfect',\n",
       "  'made',\n",
       "  'sit',\n",
       "  'outsid',\n",
       "  'overlook',\n",
       "  'ground',\n",
       "  'absolut',\n",
       "  'pleasur',\n",
       "  'waitress',\n",
       "  'excel',\n",
       "  'food',\n",
       "  'arriv',\n",
       "  'quick',\n",
       "  'semibusi',\n",
       "  'saturday',\n",
       "  'morn',\n",
       "  'look',\n",
       "  'like',\n",
       "  'place',\n",
       "  'fill',\n",
       "  'pretti',\n",
       "  'quick',\n",
       "  'earlier',\n",
       "  'get',\n",
       "  'better',\n",
       "  'favor',\n",
       "  'get',\n",
       "  'bloodi',\n",
       "  'mari',\n",
       "  'phenomen',\n",
       "  'simpli',\n",
       "  'best',\n",
       "  'ive',\n",
       "  'ever',\n",
       "  'im',\n",
       "  'pretti',\n",
       "  'sure',\n",
       "  'use',\n",
       "  'ingredi',\n",
       "  'garden',\n",
       "  'blend',\n",
       "  'fresh',\n",
       "  'order',\n",
       "  'amaz',\n",
       "  'everyth',\n",
       "  'menu',\n",
       "  'look',\n",
       "  'excel',\n",
       "  'white',\n",
       "  'truffl',\n",
       "  'scrambl',\n",
       "  'egg',\n",
       "  'veget',\n",
       "  'skillet',\n",
       "  'tasti',\n",
       "  'delici',\n",
       "  'came',\n",
       "  'piec',\n",
       "  'griddl',\n",
       "  'bread',\n",
       "  'amaz',\n",
       "  'absolut',\n",
       "  'made',\n",
       "  'meal',\n",
       "  'complet',\n",
       "  'best',\n",
       "  'toast',\n",
       "  'ive',\n",
       "  'ever',\n",
       "  'anyway',\n",
       "  'cant',\n",
       "  'wait',\n",
       "  'go',\n",
       "  'back'],\n",
       " ['idea',\n",
       "  'peopl',\n",
       "  'give',\n",
       "  'bad',\n",
       "  'review',\n",
       "  'place',\n",
       "  'goe',\n",
       "  'show',\n",
       "  'pleas',\n",
       "  'everyon',\n",
       "  'probabl',\n",
       "  'gripe',\n",
       "  'someth',\n",
       "  'faultther',\n",
       "  'mani',\n",
       "  'peopl',\n",
       "  'like',\n",
       "  'case',\n",
       "  'friend',\n",
       "  'arriv',\n",
       "  'pm',\n",
       "  'past',\n",
       "  'sunday',\n",
       "  'pretti',\n",
       "  'crowd',\n",
       "  'thought',\n",
       "  'sunday',\n",
       "  'even',\n",
       "  'thought',\n",
       "  'would',\n",
       "  'wait',\n",
       "  'forev',\n",
       "  'get',\n",
       "  'seat',\n",
       "  'said',\n",
       "  'well',\n",
       "  'seat',\n",
       "  'girl',\n",
       "  'come',\n",
       "  'back',\n",
       "  'seat',\n",
       "  'someon',\n",
       "  'els',\n",
       "  'seat',\n",
       "  'waiter',\n",
       "  'came',\n",
       "  'got',\n",
       "  'drink',\n",
       "  'order',\n",
       "  'everyon',\n",
       "  'pleasant',\n",
       "  'host',\n",
       "  'seat',\n",
       "  'us',\n",
       "  'waiter',\n",
       "  'server',\n",
       "  'price',\n",
       "  'good',\n",
       "  'well',\n",
       "  'place',\n",
       "  'order',\n",
       "  'decid',\n",
       "  'want',\n",
       "  'share',\n",
       "  'bake',\n",
       "  'spaghetti',\n",
       "  'calzon',\n",
       "  'small',\n",
       "  'here',\n",
       "  'beef',\n",
       "  'pizza',\n",
       "  'tri',\n",
       "  'calzon',\n",
       "  'huge',\n",
       "  'got',\n",
       "  'smallest',\n",
       "  'one',\n",
       "  'person',\n",
       "  'got',\n",
       "  'small',\n",
       "  'pizza',\n",
       "  'awesom',\n",
       "  'friend',\n",
       "  'like',\n",
       "  'pizza',\n",
       "  'better',\n",
       "  'like',\n",
       "  'calzon',\n",
       "  'better',\n",
       "  'calzon',\n",
       "  'sweetish',\n",
       "  'sauc',\n",
       "  'that',\n",
       "  'like',\n",
       "  'sauc',\n",
       "  'box',\n",
       "  'part',\n",
       "  'pizza',\n",
       "  'take',\n",
       "  'home',\n",
       "  'door',\n",
       "  'everyth',\n",
       "  'great',\n",
       "  'like',\n",
       "  'bad',\n",
       "  'review',\n",
       "  'goe',\n",
       "  'show',\n",
       "  'tri',\n",
       "  'thing',\n",
       "  'bad',\n",
       "  'review',\n",
       "  'serious',\n",
       "  'issu'],\n",
       " ['love', 'gyro', 'plate', 'rice', 'good', 'also', 'dig', 'candi', 'select']]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_reviews[0:3]\n",
    "#len(preprocessed_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec model from scratch with gensim using the list of lists we crated form review texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('awesom', 0.9006556868553162), ('fantast', 0.8613003492355347), ('enjoy', 0.8420926928520203), ('wonder', 0.8332855701446533), ('excel', 0.8252487778663635), ('good', 0.8063887357711792), ('amaz', 0.7996500134468079), ('cheap', 0.7953023910522461), ('hotwir', 0.7921968698501587), ('casual', 0.7865387201309204)]\n",
      "==================================================\n",
      "[('life', 0.9445294141769409), ('gotten', 0.9285500049591064), ('cleanest', 0.9175071120262146), ('bradley', 0.9172347784042358), ('jew', 0.9139502644538879), ('eaten', 0.9080661535263062), ('weve', 0.9065256714820862), ('part', 0.9053100943565369), ('tempephoenix', 0.9040247797966003), ('heard', 0.9039435386657715)]\n",
      "[ 0.25224653 -0.02100236 -0.09549011 -0.25566593  0.03660142 -0.0782662\n",
      " -0.07191881  0.22030371 -0.10144444 -0.09333372 -0.38288698  0.06886412\n",
      " -0.01381888 -0.02306023  0.13424507  0.19485225 -0.32522297  0.37036988\n",
      " -0.06445486 -0.03232805 -0.04107588  0.16701028  0.1826102   0.10448657\n",
      "  0.21379161  0.0945508  -0.12504855  0.07027788 -0.34442204 -0.21104077\n",
      "  0.1413286   0.10282076 -0.08313899  0.17549677  0.1850401  -0.07788722\n",
      "  0.12912458  0.17085743 -0.12961325 -0.02758416  0.06768646  0.01905147\n",
      " -0.26565805  0.33807647 -0.30872172  0.17235942 -0.3297288   0.02344259\n",
      " -0.14512789 -0.39073107  0.09707904  0.08800327 -0.14049533  0.48916003\n",
      " -0.19023156  0.14366823  0.3622979  -0.42505404  0.08167378 -0.05711213\n",
      "  0.17928477  0.2114009  -0.14204475  0.11144561 -0.03340414  0.1899843\n",
      "  0.34108958  0.3200388  -0.35567296 -0.35889828  0.2880577   0.14278501\n",
      " -0.07603829  0.17103976  0.12202727 -0.37564865 -0.11735627 -0.38271478\n",
      " -0.11808272  0.3158955  -0.35816452  0.07323343 -0.11545455 -0.3145444\n",
      "  0.17839915 -0.67459136  0.08972134  0.0866937  -0.23495659 -0.11062964\n",
      "  0.4696979   0.36871803  0.05363706  0.14661968 -0.02200978  0.08486107\n",
      " -0.19492704 -0.21254258 -0.5710498  -0.07534349 -0.36610976  0.31172648\n",
      "  0.15099762 -0.12845527  0.0711415  -0.05144463 -0.03558312  0.03503432\n",
      " -0.02132599  0.38038102  0.3228075  -0.10180515 -0.07965857  0.04328097\n",
      " -0.04900492 -0.28136307  0.42573693 -0.16542335  0.55446887 -0.13035314\n",
      " -0.03181276  0.1671909  -0.03470532  0.12624823  0.17879142 -0.32937267\n",
      "  0.23854917 -0.16179085 -0.00225008 -0.07828212  0.37181145  0.16608784\n",
      "  0.02921598  0.19905137 -0.07635525 -0.45032358 -0.10307758  0.25325176\n",
      " -0.07942169 -0.10939888 -0.11006816 -0.14801288 -0.3819881   0.26769248\n",
      "  0.0013215   0.08374442  0.20359865  0.2236422  -0.54557246 -0.05145874\n",
      "  0.07474981 -0.37327594  0.09541295  0.05580295 -0.00648709  0.07348798\n",
      " -0.16320753 -0.45834902  0.04749401 -0.10986692 -0.22428179 -0.07302488\n",
      "  0.07910723 -0.26283213  0.04723855 -0.00288829  0.08545656 -0.2555257\n",
      " -0.18380551 -0.29869866  0.17403495 -0.00431055  0.39640385  0.24758597\n",
      "  0.12190436 -0.09593625 -0.21829124  0.05900205 -0.10207287  0.3238287\n",
      "  0.30991083  0.26690984 -0.34680855  0.28567043  0.22008102  0.03882897\n",
      "  0.08287671  0.16525897  0.37557724  0.29726818 -0.12571251  0.20347613\n",
      "  0.06306096  0.25360826 -0.14085078  0.3376488   0.2031906   0.1520262\n",
      "  0.20050398 -0.00186427  0.06560578  0.12174454  0.23313686 -0.32077497\n",
      " -0.04719144 -0.142627    0.45276448 -0.08193035 -0.28707734  0.69702584\n",
      " -0.05519794  0.11014662  0.03461483 -0.14357829 -0.19230312 -0.16355464\n",
      " -0.08584326  0.08760025  0.42165813  0.20846702 -0.31659254  0.04487467\n",
      "  0.07181662  0.3640315   0.05083967  0.18576819  0.46944776 -0.01733411\n",
      "  0.16705742 -0.16573335  0.20796178 -0.67352265 -0.12239059 -0.37492785\n",
      "  0.35872737  0.05100536  0.16470303  0.16948763 -0.25382063 -0.16530547\n",
      "  0.23231784 -0.30869702 -0.37691528  0.02539887 -0.20113967  0.06897753\n",
      " -0.15510015  0.03249573  0.10673983  0.1330877   0.19682127  0.23441063\n",
      "  0.10953596  0.03055449  0.36735424 -0.02090849 -0.14680828  0.19373262\n",
      "  0.38874894 -0.06964418  0.08216365 -0.10963705 -0.18743087  0.31021678\n",
      "  0.11730375  0.03558154  0.05925827  0.17884237 -0.23084158  0.42975596\n",
      "  0.07799945 -0.1312743  -0.36473173  0.08702932  0.08778352 -0.1491749\n",
      "  0.02636242  0.06682929 -0.12310553  0.32897028  0.26910013  0.16875759\n",
      " -0.04318378  0.31934443 -0.06262477  0.07778238  0.00451883 -0.12144672\n",
      " -0.10979147  0.5022825   0.00919368  0.05938569 -0.19609463 -0.01546486\n",
      "  0.04804096  0.1475792   0.06627181  0.10736771 -0.05510719  0.2026756 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "# min_count = 2 considers only words that occured atleast 2 times\n",
    "w2v_model=Word2Vec(preprocessed_reviews,min_count=2,size=300)\n",
    "print(w2v_model.wv.most_similar('great'))\n",
    "print('='*50)\n",
    "print(w2v_model.wv.most_similar('worst'))\n",
    "print(w2v_model.wv['worst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dilla', 0.9426113367080688), ('husband', 0.9396105408668518), ('marti', 0.9355227947235107), ('dad', 0.9271290302276611), ('benni', 0.9196608066558838), ('boyfriend', 0.9152238368988037), ('boop', 0.9145913124084473), ('carbonara', 0.9131380319595337), ('goround', 0.9117599725723267), ('campanell', 0.9076445698738098)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.most_similar('wife'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all the w2vec words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12122\n"
     ]
    }
   ],
   "source": [
    "w2v_words=list(w2v_model.wv.vocab)\n",
    "print(len(w2v_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a MeanEmbeddingVectroizer which uses  Avg w2v to convert sentenses into vectors(from all words in sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/ \n",
    "from tqdm import tqdm\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec[0])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "        for sent in tqdm(X): # for each review/sentence\n",
    "            sent_vec = np.zeros(300) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "            cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "            for word in sent: # for each word in a review/sentence\n",
    "                if word in w2v_words:\n",
    "                    vec = w2v_model.wv[word]\n",
    "                    sent_vec += vec\n",
    "                    cnt_words += 1\n",
    "            if cnt_words != 0:\n",
    "                sent_vec /= cnt_words\n",
    "            sent_vectors.append(sent_vec)\n",
    "        return sent_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets split the input data to contain review with starts of 5 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data[(data['stars']==5 )| (data['stars']==1)]\n",
    "X_W2V= df['text']\n",
    "y_W2V=df['stars']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Input features/text using custom analyzer we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4086/4086 [02:02<00:00, 33.23it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "for sentence in tqdm(X_W2V):\n",
    "    sentence=clean_review_text(sentence)\n",
    "    clean_text.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4086\n",
      "[['wife', 'took', 'birthday', 'breakfast', 'excel', 'weather', 'perfect', 'made', 'sit', 'outsid', 'overlook', 'ground', 'absolut', 'pleasur', 'waitress', 'excel', 'food', 'arriv', 'quick', 'semibusi', 'saturday', 'morn', 'look', 'like', 'place', 'fill', 'pretti', 'quick', 'earlier', 'get', 'better', 'favor', 'get', 'bloodi', 'mari', 'phenomen', 'simpli', 'best', 'ive', 'ever', 'im', 'pretti', 'sure', 'use', 'ingredi', 'garden', 'blend', 'fresh', 'order', 'amaz', 'everyth', 'menu', 'look', 'excel', 'white', 'truffl', 'scrambl', 'egg', 'veget', 'skillet', 'tasti', 'delici', 'came', 'piec', 'griddl', 'bread', 'amaz', 'absolut', 'made', 'meal', 'complet', 'best', 'toast', 'ive', 'ever', 'anyway', 'cant', 'wait', 'go', 'back'], ['idea', 'peopl', 'give', 'bad', 'review', 'place', 'goe', 'show', 'pleas', 'everyon', 'probabl', 'gripe', 'someth', 'faultther', 'mani', 'peopl', 'like', 'case', 'friend', 'arriv', 'pm', 'past', 'sunday', 'pretti', 'crowd', 'thought', 'sunday', 'even', 'thought', 'would', 'wait', 'forev', 'get', 'seat', 'said', 'well', 'seat', 'girl', 'come', 'back', 'seat', 'someon', 'els', 'seat', 'waiter', 'came', 'got', 'drink', 'order', 'everyon', 'pleasant', 'host', 'seat', 'us', 'waiter', 'server', 'price', 'good', 'well', 'place', 'order', 'decid', 'want', 'share', 'bake', 'spaghetti', 'calzon', 'small', 'here', 'beef', 'pizza', 'tri', 'calzon', 'huge', 'got', 'smallest', 'one', 'person', 'got', 'small', 'pizza', 'awesom', 'friend', 'like', 'pizza', 'better', 'like', 'calzon', 'better', 'calzon', 'sweetish', 'sauc', 'that', 'like', 'sauc', 'box', 'part', 'pizza', 'take', 'home', 'door', 'everyth', 'great', 'like', 'bad', 'review', 'goe', 'show', 'tri', 'thing', 'bad', 'review', 'serious', 'issu']]\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_text))\n",
    "print(clean_text[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4086"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_W2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the cleaned Input data into vectors using avg W2V (MeanEmbeddingVectroizer we created above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4086/4086 [00:19<00:00, 214.34it/s]\n"
     ]
    }
   ],
   "source": [
    "sent_vectors_input=[[]]\n",
    "model=MeanEmbeddingVectorizer(clean_text)\n",
    "sent_vectors_input=model.transform(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the input data into Traning and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_W2V, X_test_W2V, y_train_W2V, y_test_W2V = train_test_split(sent_vectors_input,y_W2V, test_size=0.2, random_state=101)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the cleaned test data into vectors using avg W2V (MeanEmbeddingVectroizer we created above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert teest data into mean vectors which \n",
    "# sent_vectors_test=[[]]\n",
    "# model=MeanEmbeddingVectorizer(X_test_W2V)\n",
    "# sent_vectors_test=model.transform(X_test_W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_W2V, X_test_W2V, y_train_W2V, y_test_W2V = train_test_split(clean_text,y_W2V, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert the training data into mean vectors\n",
    "# sent_vectors_train=[[]]\n",
    "# model=MeanEmbeddingVectorizer(X_train_W2V)\n",
    "# sent_vectors_train=model.transform(X_train_W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert teest data into mean vectors which \n",
    "# sent_vectors_test=[[]]\n",
    "# model=MeanEmbeddingVectorizer(X_test_W2V)\n",
    "# sent_vectors_test=model.transform(X_test_W2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets find the optimal K value using 10 fold cross validaiton score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors  are: [0.1900208251252321, 0.1606517701356449, 0.15116508133055662, 0.1536087503048723, 0.1508620851391156, 0.14780304309487635, 0.14872328849364924, 0.14872328849364946, 0.1496444719611265, 0.15025796889364185, 0.14719705071199418, 0.145972871053076, 0.149338661563573]\n",
      "Least Error is : 0.145972871053076\n",
      "k value to use is:  23\n"
     ]
    }
   ],
   "source": [
    "#lets keep the neighbours odd number form 1 to 25 \n",
    "n_neigh=[i for i in range(26) if i%2!=0]\n",
    "cv_scores=[]\n",
    "#For each k value \n",
    "#1.Split the same training data with 10 fold and try to find the best K vlue .It returs list of scores.\n",
    "#2.compute the mean score for each this neighbor value and store it\n",
    "for i in n_neigh:\n",
    "    model=KNeighborsClassifier(n_neighbors=i,weights='distance')\n",
    "    scores = cross_val_score(model, X_train_W2V, y_train_W2V, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "#Compute the Error for each k value and print the kvalue that has minimum error\n",
    "MSE=[1-scr for scr in cv_scores] \n",
    "print(\"Errors  are:\",MSE)\n",
    "print(\"Least Error is :\", min(MSE))\n",
    "print(\"k value to use is: \",n_neigh[MSE.index(min(MSE))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 58  92]\n",
      " [ 11 657]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.39      0.53       150\n",
      "           5       0.88      0.98      0.93       668\n",
      "\n",
      "    accuracy                           0.87       818\n",
      "   macro avg       0.86      0.69      0.73       818\n",
      "weighted avg       0.87      0.87      0.85       818\n",
      "\n",
      "0.8740831295843521\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_KNN=KNeighborsClassifier(n_neighbors=23,weights='distance')\n",
    "model_KNN.fit(X_train_W2V, y_train_W2V)\n",
    "predictions_KN=model_KNN.predict(X_test_W2V)\n",
    "\n",
    "#Analyze Results\n",
    "print(confusion_matrix(y_test_W2V,predictions_KN))\n",
    "print(classification_report(y_test_W2V,predictions_KN))\n",
    "print(accuracy_score(y_test_W2V,predictions_KN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets use RF classifer to compare results with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73  77]\n",
      " [ 24 644]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.49      0.59       150\n",
      "           5       0.89      0.96      0.93       668\n",
      "\n",
      "    accuracy                           0.88       818\n",
      "   macro avg       0.82      0.73      0.76       818\n",
      "weighted avg       0.87      0.88      0.87       818\n",
      "\n",
      "0.8765281173594132\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train_RF_W2V, X_test_RF_W2V, y_train_RF_W2V, y_test_RF_W2V = train_test_split(sent_vectors_input,y_W2V, test_size=0.2, random_state=101)\n",
    "\n",
    "\n",
    "model1=RandomForestClassifier()\n",
    "model1.fit(X_train_RF_W2V, y_train_RF_W2V)\n",
    "predictions_RF=model1.predict(X_test_RF_W2V)\n",
    "\n",
    "print(confusion_matrix(y_test_RF_W2V,predictions_RF))\n",
    "print(classification_report(y_test_RF_W2V,predictions_RF))\n",
    "print(accuracy_score(y_test_RF_W2V,predictions_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use TF-IDF W2V  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fist compute the TF-IDF W2V for each word from the cleaned reviews with stars as 5 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of list into list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_string=list(map(' '.join,preprocessed_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wife took birthday breakfast excel weather perfect made sit outsid overlook ground absolut pleasur waitress excel food arriv quick semibusi saturday morn look like place fill pretti quick earlier get better favor get bloodi mari phenomen simpli best ive ever im pretti sure use ingredi garden blend fresh order amaz everyth menu look excel white truffl scrambl egg veget skillet tasti delici came piec griddl bread amaz absolut made meal complet best toast ive ever anyway cant wait go back',\n",
       " 'idea peopl give bad review place goe show pleas everyon probabl gripe someth faultther mani peopl like case friend arriv pm past sunday pretti crowd thought sunday even thought would wait forev get seat said well seat girl come back seat someon els seat waiter came got drink order everyon pleasant host seat us waiter server price good well place order decid want share bake spaghetti calzon small here beef pizza tri calzon huge got smallest one person got small pizza awesom friend like pizza better like calzon better calzon sweetish sauc that like sauc box part pizza take home door everyth great like bad review goe show tri thing bad review serious issu']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_string[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the TF-IDF values for each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x25512 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 546478 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectroizer =TfidfVectorizer()\n",
    "tf_idf_vectroizer.fit_transform(reviews_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a dictionary which will contain each feature/word as key and corresponding TF-IDF as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict(zip(tf_idf_vectroizer.get_feature_names(), list(tf_idf_vectroizer.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.4186809 , 9.11182808, 9.51729319, ..., 9.51729319, 9.51729319,\n",
       "       9.51729319])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectroizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.517293186416572\n",
      "1.7792408887272548\n"
     ]
    }
   ],
   "source": [
    "print(max(tf_idf_vectroizer.idf_))\n",
    "print(min(tf_idf_vectroizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4086/4086 [02:45<00:00, 24.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = tf_idf_vectroizer.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in tqdm(clean_text): # for each review/sentence \n",
    "    sent_vec = np.zeros(300) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are \n",
    "            # dictionary[word] = idf value of word in whole courpus\n",
    "            # sent.count(word) = tf valeus of word in this review\n",
    "            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.17523705e-01,  5.93179261e-02,  6.01419858e-02, -2.71541181e-01,\n",
       "        -2.51580368e-01, -2.57317663e-01, -1.30837711e-01, -5.94815061e-02,\n",
       "        -1.16868554e-01, -5.90304903e-02, -3.26315937e-01,  1.71290373e-01,\n",
       "        -1.71241059e-01,  1.12406905e-01, -7.17380385e-02,  6.64943500e-02,\n",
       "        -1.31141760e-01,  2.08006331e-01, -1.51114679e-01, -6.19295455e-02,\n",
       "         1.22057865e-01, -1.44803749e-01, -1.94574314e-01,  1.14193521e-01,\n",
       "         1.48872522e-01,  1.92155831e-01, -1.44985800e-01, -1.30655096e-01,\n",
       "        -4.35410594e-01, -2.98753940e-01,  1.71577481e-01, -1.24333487e-01,\n",
       "         1.77292220e-01,  4.30290326e-04,  4.75290782e-02, -6.17484342e-02,\n",
       "        -9.68141151e-02, -4.00205664e-03, -1.85403260e-01,  6.75381966e-02,\n",
       "        -4.05337015e-03, -2.44661675e-03,  2.48271722e-02,  3.87210883e-01,\n",
       "         5.66518724e-02,  2.96917530e-01, -4.32251723e-01, -1.93252636e-02,\n",
       "        -1.83223170e-01, -3.05307707e-01, -1.26852014e-01, -6.10682554e-02,\n",
       "        -1.90232697e-01,  6.23079015e-01, -2.11299989e-01,  2.09887978e-01,\n",
       "         2.34327980e-01, -1.58114207e-01,  6.50147477e-02,  4.96185121e-02,\n",
       "         1.23251413e-01,  2.67840347e-01, -3.23789932e-01,  1.37052127e-01,\n",
       "        -1.11808750e-01,  1.55512822e-01, -9.93340497e-02,  2.44547617e-01,\n",
       "        -2.44967633e-01, -2.17826680e-01,  1.32045701e-01,  1.75486207e-01,\n",
       "        -1.60731719e-01, -7.44637730e-02,  1.60701567e-01, -3.07882755e-01,\n",
       "        -1.51908131e-01, -1.85072226e-01, -1.40249506e-01,  1.11431729e-01,\n",
       "        -3.31581525e-01, -8.15606238e-02,  4.67689641e-02, -1.88747875e-01,\n",
       "         1.30416513e-01, -3.48217532e-01, -3.18796555e-02, -7.53438665e-03,\n",
       "        -6.28735686e-02, -1.15729623e-01,  3.85130029e-01,  3.57539275e-01,\n",
       "         1.14547960e-01, -2.67252158e-01, -7.71802892e-02,  1.55774759e-01,\n",
       "        -8.33326284e-02,  4.90389109e-02, -2.80004536e-01, -7.94553385e-02,\n",
       "        -4.35546269e-02,  1.37893631e-01,  3.75181242e-02,  1.75721076e-01,\n",
       "         3.79068498e-01, -2.55199835e-01, -1.38249226e-01,  5.81393298e-03,\n",
       "        -9.47493818e-03,  2.56059148e-01,  9.70632361e-02, -2.49393400e-01,\n",
       "        -1.11808388e-01, -1.21374749e-01,  6.16790223e-02, -2.58087857e-01,\n",
       "         3.23361330e-01, -9.24463575e-02,  5.72548122e-01,  1.98620214e-01,\n",
       "         9.65754059e-02,  2.12005700e-01,  1.50254742e-01,  1.23391634e-01,\n",
       "         1.00110145e-01, -9.02378392e-05,  2.01785678e-01,  1.65244824e-01,\n",
       "        -2.20132004e-01, -7.01345989e-02,  6.53062312e-02,  1.64856126e-01,\n",
       "         1.07657022e-01,  2.33490160e-01, -8.71707833e-02, -3.37645690e-01,\n",
       "         6.94336207e-02,  2.15258868e-01, -5.51625514e-02, -3.39007867e-01,\n",
       "        -1.56992154e-01, -1.52986175e-01, -1.48180294e-02, -5.30238015e-02,\n",
       "         2.15467282e-02,  2.67884339e-02,  3.52315566e-02,  1.73679533e-01,\n",
       "        -4.25105843e-01,  3.79234551e-03,  8.83213898e-02, -2.29009019e-01,\n",
       "         8.36750855e-02,  6.11486363e-02, -7.81440117e-02, -7.17670025e-02,\n",
       "         5.76394734e-02, -2.56653507e-01,  1.82392763e-01,  1.11382496e-01,\n",
       "         9.45084268e-03, -1.79899133e-02,  2.28517586e-01, -3.44889953e-01,\n",
       "        -4.55570199e-02, -1.15157620e-03,  5.36229840e-02, -4.92618766e-02,\n",
       "        -2.42349938e-01, -3.30532469e-01,  3.89332236e-01,  9.22999209e-02,\n",
       "         1.95139041e-01,  1.83269088e-01,  1.46083761e-01, -5.50266141e-02,\n",
       "        -9.07590784e-02,  1.30451863e-01, -8.53365985e-02,  4.12434545e-01,\n",
       "         1.64029771e-01,  1.37933678e-01, -1.93779643e-01,  1.63686798e-01,\n",
       "         2.75579346e-01, -8.75207537e-02,  1.65146853e-01,  4.73087342e-02,\n",
       "         2.23502546e-01,  1.91617822e-01, -8.10390276e-02,  3.84077094e-01,\n",
       "         3.24599184e-02,  2.17630213e-01, -3.85926595e-02,  2.30601565e-01,\n",
       "         1.83504237e-01,  1.12769112e-01,  1.74343313e-01, -5.99618162e-02,\n",
       "         1.39767494e-01,  2.46938475e-01,  3.45039152e-01, -1.10713626e-01,\n",
       "        -1.46030115e-01, -7.62237286e-03, -6.60634140e-02,  1.45242087e-02,\n",
       "        -1.63250769e-01,  2.84402614e-01,  6.25191818e-03,  1.38003190e-01,\n",
       "         4.49183685e-01,  9.15483183e-02,  5.48638719e-02, -7.26912996e-02,\n",
       "        -1.05182170e-02, -1.44059066e-01,  3.86129789e-01,  1.31937894e-01,\n",
       "        -3.21985062e-01,  1.01352407e-01, -1.02432296e-01,  1.91181682e-01,\n",
       "        -2.34590798e-01,  4.00593803e-02,  3.00250968e-01, -1.07655067e-01,\n",
       "        -1.30384651e-02,  4.11423480e-02, -1.24580118e-03, -3.08603511e-01,\n",
       "         1.60243502e-02, -2.65072234e-01,  3.73650133e-01, -7.34705573e-02,\n",
       "         1.06046581e-01,  4.98269373e-02, -8.54724698e-02,  3.36750930e-03,\n",
       "         1.64961814e-01, -2.52897271e-01, -3.41901251e-01,  9.42382306e-02,\n",
       "        -7.84345951e-02, -1.02992637e-01,  9.00597187e-02, -3.12201704e-02,\n",
       "        -1.42589519e-01, -7.07002461e-02,  3.63076029e-01,  1.76666935e-01,\n",
       "         1.05267087e-01,  3.80116081e-02,  3.66045550e-01,  9.91914433e-02,\n",
       "        -2.71521471e-01, -1.03958527e-02,  1.86173934e-01, -5.52030942e-03,\n",
       "        -1.03173401e-01, -1.31478040e-01, -3.46953211e-03, -7.95172799e-02,\n",
       "        -7.91085668e-02, -2.19417115e-01, -1.77060459e-01,  1.21010759e-01,\n",
       "        -1.57508331e-01,  2.45855057e-01, -1.32958839e-02, -6.81330291e-02,\n",
       "        -1.06109728e-01,  4.09895516e-02,  2.76699537e-01, -4.72439702e-02,\n",
       "         4.41362854e-02,  1.82608740e-01, -1.37922187e-01,  3.03163234e-01,\n",
       "         1.93458390e-01,  1.71390504e-01, -9.32700459e-02,  1.35776548e-01,\n",
       "        -1.33822713e-01,  2.36309070e-01,  8.83056741e-02, -9.05668537e-03,\n",
       "        -2.24337499e-01,  2.82515980e-01, -2.37814812e-01, -1.37187658e-01,\n",
       "         2.46130427e-03,  3.16183503e-01,  8.15858791e-02,  1.07224586e-01,\n",
       "        -1.95945311e-02, -4.64402707e-03, -2.89717972e-02,  2.50196811e-01]),\n",
       " array([-2.84301932e-02,  1.27046834e-01,  8.90242126e-02, -2.66358423e-01,\n",
       "        -2.28025801e-01, -2.12081288e-01, -6.16971249e-03, -1.84014333e-01,\n",
       "         2.51317766e-02,  3.38334970e-02, -2.06240621e-01,  6.51476169e-02,\n",
       "         1.49426319e-02, -7.25728534e-03,  9.43808145e-02, -2.29348256e-01,\n",
       "        -8.18367986e-03,  4.05599497e-01, -2.05097990e-01, -2.12964663e-02,\n",
       "         1.39697212e-01, -1.74297959e-01, -2.18665725e-01,  8.42874102e-02,\n",
       "         8.00413334e-03,  1.99335967e-01, -2.02649711e-01, -1.35238973e-01,\n",
       "        -3.59336192e-01, -2.89487525e-01,  1.55019800e-01, -1.25612667e-01,\n",
       "         9.99196386e-02,  3.28947431e-02,  2.77815722e-03,  1.56546806e-02,\n",
       "        -1.36816955e-01, -7.80672073e-02, -3.72749079e-01, -8.98242439e-02,\n",
       "         4.69770359e-02,  1.87791869e-02,  1.11294801e-02,  3.34941354e-01,\n",
       "         8.28590224e-05,  3.05453337e-01, -4.25496124e-01,  1.29235924e-01,\n",
       "        -1.06210596e-01, -1.50054294e-01, -2.21126488e-01,  1.66135346e-02,\n",
       "        -1.79481107e-01,  5.05711940e-01, -1.99678362e-01,  1.25048381e-01,\n",
       "         6.80870478e-02,  3.95237310e-02, -1.70100770e-02, -7.73956025e-02,\n",
       "         2.29285509e-02,  2.79239436e-01, -1.26930204e-01,  6.69560635e-02,\n",
       "        -2.40463711e-01, -4.60886022e-02, -1.86038608e-01,  3.05691662e-01,\n",
       "        -3.20899512e-01, -1.07469436e-01,  4.35007282e-02,  2.74117021e-01,\n",
       "         2.90852402e-02, -1.92233530e-01,  1.19596047e-01, -2.52268912e-01,\n",
       "        -3.06277375e-02, -2.25490542e-01, -7.88988497e-02,  6.87856005e-02,\n",
       "        -2.75120801e-01, -8.24083858e-02, -6.65296889e-02, -1.95000165e-01,\n",
       "         1.21948875e-01, -2.27609553e-01, -9.68572208e-02,  5.05378589e-02,\n",
       "        -5.51204239e-03, -1.36843881e-01,  3.35092039e-01,  3.10271893e-01,\n",
       "         1.92123408e-01, -2.05459673e-01, -2.55411538e-01,  9.06817374e-02,\n",
       "        -5.48889652e-02, -1.05184140e-01, -2.72735485e-02, -4.34939584e-02,\n",
       "        -1.50796442e-02,  1.13837138e-01, -6.32439581e-02,  8.67692999e-02,\n",
       "         4.00950738e-01, -1.54027351e-01, -2.54065199e-01, -7.09368354e-02,\n",
       "        -1.85991992e-02,  3.60853900e-01, -9.90794880e-02, -1.78504298e-01,\n",
       "        -2.80974153e-01,  1.06907325e-01,  2.03873405e-02, -3.84877125e-01,\n",
       "         4.29835994e-01, -1.25791454e-01,  5.21420385e-01, -7.94194972e-02,\n",
       "         1.95271644e-01,  1.31426726e-01,  1.67101734e-01, -5.71586624e-02,\n",
       "         2.57875441e-01,  4.55069261e-02,  1.58451382e-01,  1.83523993e-01,\n",
       "        -2.46609957e-01, -5.45842156e-02,  1.47928380e-01,  3.00537723e-01,\n",
       "         2.15840073e-01,  1.81230050e-01, -3.38054502e-01, -2.74570737e-01,\n",
       "         2.03192277e-01,  4.09044480e-01, -2.17354892e-01, -2.56170269e-01,\n",
       "        -7.02491507e-02,  1.15500169e-02,  5.36838065e-02, -1.07994547e-01,\n",
       "         5.13672492e-02,  4.67418692e-02,  1.47380534e-01,  2.23384802e-01,\n",
       "        -5.13552277e-01,  4.01132073e-02,  1.72566213e-01, -2.12351041e-01,\n",
       "        -1.32631687e-01,  9.89748386e-02, -5.66648755e-02, -2.59475852e-01,\n",
       "         7.52236707e-02, -1.15088660e-01,  1.35664673e-01,  1.27007953e-01,\n",
       "         7.67081315e-03, -8.75574095e-02,  1.75030713e-01, -3.19914476e-01,\n",
       "        -6.48511790e-03,  1.21725792e-01,  5.17633173e-02, -1.23032332e-01,\n",
       "        -7.38645598e-02, -1.43048147e-01,  3.44953531e-01, -2.65380954e-02,\n",
       "         2.44742602e-01, -5.84795733e-02,  2.30417885e-01, -7.56759221e-02,\n",
       "        -2.28944591e-01,  1.89786050e-01, -1.32532338e-01,  1.97232020e-01,\n",
       "         1.69592975e-01,  3.00098455e-02, -1.28909549e-01, -8.19006608e-02,\n",
       "         3.73154910e-01, -1.31222410e-01,  8.99192798e-02,  6.07437946e-02,\n",
       "         2.83609933e-01,  1.43702971e-01, -2.47381627e-02,  2.34239969e-01,\n",
       "         9.87836067e-02,  1.48776935e-01,  8.74749372e-02,  1.87610095e-01,\n",
       "         3.15597593e-01,  1.69395287e-01,  1.29688262e-01, -6.62990306e-02,\n",
       "         2.52605081e-01,  2.23937942e-01,  2.53921230e-01,  1.15156861e-02,\n",
       "        -4.23145109e-04, -4.54050756e-03,  1.06557819e-01, -1.25253348e-01,\n",
       "        -2.13895296e-01,  2.47371703e-01,  1.42186413e-02,  2.34025697e-01,\n",
       "         5.85390793e-01,  2.79416835e-02,  6.33657499e-02,  1.52713559e-01,\n",
       "        -9.48199311e-02, -1.03522691e-01,  3.81996336e-01,  1.34291627e-01,\n",
       "        -2.90333324e-01, -5.16303379e-02, -4.16654681e-02,  2.47847383e-01,\n",
       "        -2.54137605e-01,  2.70391673e-02,  3.41693189e-01, -5.27966374e-02,\n",
       "        -1.14548159e-01,  9.25173313e-02, -8.93951204e-02, -4.14611710e-01,\n",
       "        -1.77419709e-02, -1.36370877e-01,  5.25294246e-01, -2.48616910e-01,\n",
       "         1.55242910e-01,  7.45539405e-02, -8.89060490e-02,  4.54407459e-02,\n",
       "         2.08627327e-01, -1.59443255e-01, -3.26805996e-01,  3.51586638e-02,\n",
       "        -1.39442574e-01, -1.64736238e-01,  9.69755967e-02,  2.26713490e-02,\n",
       "        -8.46897365e-02, -2.16241005e-01,  3.98604411e-01, -1.23167676e-02,\n",
       "        -2.13050790e-02,  2.18635116e-01,  3.75211867e-01,  2.88136106e-01,\n",
       "        -1.97945085e-01, -9.19931965e-02,  1.36920938e-01,  1.70017209e-02,\n",
       "        -5.52467554e-02, -6.81132477e-02, -9.08634805e-02, -2.47732978e-02,\n",
       "        -9.35927632e-02, -9.49108490e-02, -1.37399684e-01,  1.52900804e-02,\n",
       "        -2.04997180e-01,  7.15141790e-02, -1.03491180e-01, -3.58651444e-03,\n",
       "        -1.87001895e-01, -5.81278750e-03,  2.29964872e-01, -1.12975585e-01,\n",
       "        -2.02462539e-02,  1.84196187e-01, -1.07419205e-01,  2.26443824e-01,\n",
       "         2.20118405e-01,  1.38641879e-01, -5.73992944e-02,  2.09706100e-01,\n",
       "        -9.50991416e-05,  4.06154619e-01,  9.10673799e-02, -4.63656193e-02,\n",
       "        -2.26480349e-01,  1.57322317e-01, -1.54666197e-01, -9.28197284e-02,\n",
       "        -5.27568557e-02,  1.47262469e-01,  7.65002673e-02,  8.00121824e-02,\n",
       "         4.84060752e-02,  3.60853243e-02, -1.79321481e-01, -2.56837419e-03])]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sent_vectors[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4086"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(tfidf_sent_vectors)\n",
    "len(y_W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TW2V, X_test_TW2V, y_train_TW2V, y_test_TW2V = train_test_split(tfidf_sent_vectors,y_W2V, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103 125]\n",
      " [ 33 965]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.45      0.57       228\n",
      "           5       0.89      0.97      0.92       998\n",
      "\n",
      "    accuracy                           0.87      1226\n",
      "   macro avg       0.82      0.71      0.75      1226\n",
      "weighted avg       0.86      0.87      0.86      1226\n",
      "\n",
      "0.8711256117455138\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model2=RandomForestClassifier()\n",
    "model2.fit(X_train_TW2V, y_train_TW2V)\n",
    "predictions_TRF=model2.predict(X_test_TW2V)\n",
    "\n",
    "print(confusion_matrix(y_test_TW2V,predictions_TRF))\n",
    "print(classification_report(y_test_TW2V,predictions_TRF))\n",
    "print(accuracy_score(y_test_TW2V,predictions_TRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 71 157]\n",
      " [ 15 983]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.31      0.45       228\n",
      "           5       0.86      0.98      0.92       998\n",
      "\n",
      "    accuracy                           0.86      1226\n",
      "   macro avg       0.84      0.65      0.69      1226\n",
      "weighted avg       0.86      0.86      0.83      1226\n",
      "\n",
      "0.8597063621533442\n"
     ]
    }
   ],
   "source": [
    "X_train_TW2V, X_test_TW2V, y_train_TW2V, y_test_TW2V = train_test_split(tfidf_sent_vectors,y_W2V, test_size=0.3, random_state=101)\n",
    "\n",
    "model_KNN=KNeighborsClassifier(n_neighbors=23,weights='distance')\n",
    "model_KNN.fit(X_train_TW2V, y_train_TW2V)\n",
    "predictions_KN=model_KNN.predict(X_test_TW2V)\n",
    "\n",
    "#Analyze Results\n",
    "print(confusion_matrix(y_test_TW2V,predictions_KN))\n",
    "print(classification_report(y_test_TW2V,predictions_KN))\n",
    "print(accuracy_score(y_test_TW2V,predictions_KN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets test using  Naive Bayes  . As the Data is impalanced lets use fit_piror=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X_W2V has reviews text with stars=1 or 5 , y_W2V has class labels 1 or 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_NB, X_test_NB, y_train_NB, y_test_NB = train_test_split(X_W2V,y_W2V, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[183  45]\n",
      " [ 38 960]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.80      0.82       228\n",
      "           5       0.96      0.96      0.96       998\n",
      "\n",
      "    accuracy                           0.93      1226\n",
      "   macro avg       0.89      0.88      0.89      1226\n",
      "weighted avg       0.93      0.93      0.93      1226\n",
      "\n",
      "0.932300163132137\n"
     ]
    }
   ],
   "source": [
    "pipeline_NB=Pipeline([('countvec',CountVectorizer()),#analyzer=clean_review_text\n",
    "              #      ('tfidf',TfidfTransformer()),\n",
    "                     ('algorithm',MultinomialNB(alpha=1,class_prior=[.8, .2]))])#alpha=1000,fit_prior=False\n",
    "\n",
    "pipeline_NB.fit(X_train_NB,y_train_NB)\n",
    "\n",
    "predictions_NB=pipeline_NB.predict(X_test_NB)\n",
    "\n",
    "#Analyze Results\n",
    "print(confusion_matrix(y_test_NB,predictions_NB))\n",
    "print(classification_report(y_test_NB,predictions_NB))\n",
    "print(accuracy_score(y_test_NB,predictions_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation: we see if we do not use TFIDF transforme the model is working better . fit_piror = False imporves  performance little bit as the data is imbalanced. We also see if we increase the alpha value the model performance reduces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
